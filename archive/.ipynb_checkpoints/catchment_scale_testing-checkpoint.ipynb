{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Catchment-scale forecasts of veg condition\n",
    "\n",
    "Using the pre-computed NDVI quarterly climatologies (these have been calculated over all of NSW, but only a subset from the Gwdir catchment is stored on the Sandbox), see if we can predict NDVI one-month in advance.\n",
    "\n",
    "- Rescaling datasets to 210x210m to speed up testing. \n",
    "\n",
    "TODO:\n",
    "- Run seasonal anomaly using `seasonal_anomalies.ipynb` and compare to anomaly calculation in this script. This will provide a validation of the code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install xarray --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datacube import Datacube\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas.plotting import lag_plot\n",
    "from pandas.plotting import autocorrelation_plot\n",
    "from datacube.utils import geometry\n",
    "from scipy import stats, signal\n",
    "from datacube.utils.geometry import assign_crs\n",
    "from statsmodels.tsa.ar_model import AutoReg\n",
    "from odc.algo import xr_reproject\n",
    "import sys\n",
    "import os\n",
    "from datacube.utils.cog import write_cog\n",
    "\n",
    "sys.path.append('../dea-notebooks/Scripts')\n",
    "from dea_datahandling import load_ard\n",
    "from dea_plotting import map_shapefile\n",
    "from dea_dask import create_local_dask_cluster\n",
    "from dea_temporal_statistics import fast_completion, smooth\n",
    "from dea_spatialtools import xr_rasterize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_local_dask_cluster()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_tifs = 'data/climatologies/mean/'\n",
    "std_tifs = 'data/climatologies/std/'\n",
    "\n",
    "shp = 'data/mdb_shps/GWYDIR RIVER.shp'\n",
    "time = ('2014-01', '2018-12')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = gpd.read_file(shp).to_crs('EPSG:4326')\n",
    "map_shapefile(gdf, attribute='BNAME')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geom = geometry.Geometry(\n",
    "        gdf.geometry.values[0].__geo_interface__, geometry.CRS(\n",
    "            'epsg:4326'))\n",
    "\n",
    "dc = Datacube(app='whatevrr')\n",
    "\n",
    "query = {\"geopolygon\": geom,\n",
    "         'time': time,\n",
    "         'measurements':['nbart_red', 'nbart_nir'],\n",
    "         'output_crs' :'EPSG:3577',\n",
    "         'resolution' : (-120, 120),\n",
    "         'resampling' :{\"fmask\": \"nearest\", \"*\": \"bilinear\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_ard(dc=dc, \n",
    "              dask_chunks={'x':750, 'y':750, 'time':-1},\n",
    "              products=[\"ga_ls8c_ard_3\"],\n",
    "              group_by=\"solar_day\",\n",
    "              **query)\n",
    "\n",
    "mask = xr_rasterize(gdf.iloc[[0]], ds)\n",
    "\n",
    "ds = ds.where(mask).astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate NDVI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = (ds.nbart_nir - ds.nbart_red) / (ds.nbart_nir + ds.nbart_red)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fill gaps and resample to monthly\n",
    "\n",
    "TODO: switch to `fast_completion` (wrap function inside xr.map_blocks or xr.apply_ufunc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndvi_monthly = ds.interpolate_na(dim='time', method='linear').resample(time='1M').mean()#.rolling(time=3, min_periods=3, center=True).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load climatologies\n",
    "Data is a subset over the Gwydir catchment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_mean_tifs = os.listdir(mean_tifs)\n",
    "list_of_std_tifs = os.listdir(std_tifs)\n",
    "\n",
    "chunks = {'x':1000, 'y':1000}\n",
    "# quarterly NDVI mean climatologies\n",
    "x = []\n",
    "for tif in list_of_mean_tifs:\n",
    "    y = assign_crs(xr.open_rasterio(mean_tifs+tif, chunks=chunks))\n",
    "    y = xr_reproject(y,ds.geobox,\"bilinear\")\n",
    "    x.append(y.rename(tif[15:-11]))\n",
    "    \n",
    "clim_mean = xr.merge(x).squeeze().drop('band')\n",
    "\n",
    "# quarterly NDVI std. dev. climatologies\n",
    "x = []\n",
    "for tif in list_of_std_tifs:\n",
    "    y = assign_crs(xr.open_rasterio(std_tifs+tif, chunks=chunks))\n",
    "    y = xr_reproject(y,ds.geobox,\"bilinear\")\n",
    "    x.append(y.rename(tif[14:-11]))\n",
    "    \n",
    "clim_std = xr.merge(x).squeeze().drop('band')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate standardized anomalies\n",
    "\n",
    "Loop through each year+quarter and substract climatology, then rebuild time-series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First compute our arrays\n",
    "clim_std = clim_std.compute()\n",
    "clim_mean = clim_mean.compute()\n",
    "ndvi_monthly = ndvi_monthly.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clim_mean.MAM.plot(size=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TODO: BETTER HANDLE YEARS LOOP...KEEPS GOING THROUGH ENTIRE YEAR EVEN IF THERE'S ONLY A FEW MONTHS FROM THAT YEAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#define the 3-month intervals\n",
    "quarter= {'JFM': [1,2,3],\n",
    "           'FMA': [2,3,4],\n",
    "           'MAM': [3,4,5],\n",
    "           'AMJ': [4,5,6],\n",
    "           'MJJ': [5,6,7],\n",
    "           'JJA': [6,7,8],\n",
    "           'JAS': [7,8,9],\n",
    "           'ASO': [8,9,10],\n",
    "           'SON': [9,10,11],\n",
    "           'OND': [10,11,12],\n",
    "           'NDJ': [11,12,1],\n",
    "           'DJF': [12,1,2],\n",
    "                      }\n",
    "#get the unique years in ds\n",
    "years = [str(i) for i in np.unique(ndvi_monthly.time.dt.year.values)]\n",
    "\n",
    "#loop through each 3 month period and calculate the anomaly\n",
    "z=[]\n",
    "for year in years:\n",
    "    for q in quarter:\n",
    "        months = quarter.get(q)\n",
    "        if (q == \"DJF\") or (q == \"NDJ\"):\n",
    "            time=(year + \"-\" + str(months[0]), str(int(year) + 1) + \"-\" + str(months[2]))\n",
    "        else:\n",
    "            time = (year + \"-\" + str(months[0]), year + \"-\" + str(months[2]))\n",
    "        obs=ndvi_monthly.sel(time=slice(time[0], time[1])).mean('time')\n",
    "        m=clim_mean[q]\n",
    "        s=clim_std[q]\n",
    "        anom = (obs - m) / s\n",
    "        print(year+'_'+q)\n",
    "        anom.rename(year+'_'+q)\n",
    "        z.append(anom)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TODO: auto handle dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build back into time-series\n",
    "stand_anomalies=xr.concat(z, dim=pd.date_range(start='2/2014', end='2/2019', freq='M')).rename({'concat_dim':'time'})\n",
    "\n",
    "stand_anomalies.mean(['x','y']).plot(figsize=(11,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a forecast\n",
    "\n",
    "`AutoReg` doesn't like the all-NaN's slices outide the mask extent, run `stand_anomalies.fillna(-999)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = stand_anomalies.notnull().all('time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mask where its all-NaN's\n",
    "stand_anomalies = stand_anomalies.fillna(-999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_length=1\n",
    "window=20\n",
    "lags=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "def xr_autoregress(da, test_length, window, lags):\n",
    "    #dropna conveneiently with pandas\n",
    "    da =  da[~np.isnan(da)]\n",
    "    # split dataset\n",
    "    train, test = da[1:len(da)-test_length], da[len(da)-test_length:]\n",
    "    # train autoregression\n",
    "    model = AutoReg(train, lags=lags)\n",
    "    model_fit = model.fit()\n",
    "    coef = model_fit.params\n",
    "\n",
    "    # walk forward over time steps in test\n",
    "    history = train[len(train)-window:]\n",
    "    history = [history[i] for i in range(len(history))]\n",
    "\n",
    "    predictions = list()\n",
    "    for t in range(len(test)):\n",
    "        length = len(history)\n",
    "        lag = [history[i] for i in range(length-window,length)]\n",
    "        yhat = coef[0]\n",
    "        for d in range(window):\n",
    "            yhat += coef[d+1] * lag[window-d-1]\n",
    "        obs = test[t]\n",
    "        predictions.append(yhat)\n",
    "        history.append(obs) \n",
    "    \n",
    "    return np.array(predictions).flatten()\n",
    "\n",
    "predict = xr.apply_ufunc(xr_autoregress,\n",
    "                      stand_anomalies, #.chunk(dict(x=750,y=750,time=-1)),\n",
    "                      kwargs={'test_length':test_length,'window':window,'lags':window},\n",
    "                      input_core_dims=[['time']],\n",
    "                      output_core_dims=[['predictions']], \n",
    "                      output_sizes=({'predictions':test_length}),\n",
    "                      exclude_dims=set(('time',)),\n",
    "                      vectorize=True,\n",
    "                      dask=\"parallelized\",\n",
    "                      output_dtypes=[stand_anomalies.dtype]).compute()\n",
    "\n",
    "print(predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = predict.where(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict.plot(size=6, vmin=-2.0, vmax=2, cmap='BrBG')\n",
    "plt.title('Standardised NDVI Anomaly one-month prediction');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stand_anomalies=stand_anomalies.where(mask)\n",
    "stand_anomalies.isel(time=-1).plot(size=6, vmin=-2.0, vmax=2, cmap='BrBG')\n",
    "plt.title('Standardised NDVI Anomaly observation');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = predict - stand_anomalies.isel(time=-1)\n",
    "\n",
    "diff.plot(size=6, vmin=-2.0, vmax=2, cmap='RdBu')\n",
    "plt.title('Difference');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff.mean(['x','y'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Can we speed this up by manually looping through pixels in cython?\n",
    "\n",
    "    from cython.parallel import prange, parallel, threadid\n",
    "    def predict_yhat(floating [:, :, :, :] da, test_length, window, lags, [:,:,:] result, num_threads):    \n",
    "    da, test_length, window, lags\n",
    "        with nogil, parallel(num_threads=number_of_threads):\n",
    "            for row in prange(m, schedule='static'):\n",
    "                for col in range(q):\n",
    "                        # do the prediction\n",
    "                        da =  da[~np.isnan(da)]\n",
    "                        # split dataset\n",
    "                        train, test = da[1:len(da)-test_length], da[len(da)-test_length:]\n",
    "                        # train autoregression\n",
    "                        model = AutoReg(train, lags=lags)\n",
    "                        model_fit = model.fit()\n",
    "                        coef = model_fit.params\n",
    "\n",
    "                        # walk forward over time steps in test\n",
    "                        history = train[len(train)-window:]\n",
    "                        history = [history[i] for i in range(len(history))]\n",
    "\n",
    "                        predictions = list()\n",
    "                        for t in range(len(test)):\n",
    "                            length = len(history)\n",
    "                            lag = [history[i] for i in range(length-window,length)]\n",
    "                            yhat = coef[0]\n",
    "                            for d in range(window):\n",
    "                                yhat += coef[d+1] * lag[window-d-1]\n",
    "                            obs = test[t]\n",
    "                            predictions.append(yhat)\n",
    "                            history.append(obs) \n",
    "\n",
    "                        return[row, col] = np.array(predictions).flatten()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
