{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seasonal Vegetation Anomaly Forecasts\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "Understanding how the vegetated landscape responds to longer-term environmental drivers such as the El Nino Southern Oscillation (ENSO) or climate change, requires the calculation of seasonal anomalies. Seasonal anomalies subtract the long-term seasonal mean from a time-series, thus removing seasonal variability and highlighting change related to longer-term drivers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import xarray as xr\n",
    "from datacube.utils.cog import write_cog\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append('../dea-notebooks/Scripts')\n",
    "from dea_plotting import display_map, map_shapefile\n",
    "from anomalies import calculate_anomalies\n",
    "from dea_dask import create_local_dask_cluster\n",
    "from dea_classificationtools import HiddenPrints\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up local dask cluster\n",
    "\n",
    "Dask will create a local cluster of cpus for running this analysis in parallel. If you'd like to see what the dask cluster is doing, click on the hyperlink that prints after you run the cell and you can watch the cluster run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://127.0.0.1:39561</li>\n",
       "  <li><b>Dashboard: </b><a href='/user/chad/proxy/8787/status' target='_blank'>/user/chad/proxy/8787/status</a></li>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>1</li>\n",
       "  <li><b>Cores: </b>8</li>\n",
       "  <li><b>Memory: </b>61.42 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://127.0.0.1:39561' processes=1 threads=8, memory=61.42 GB>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "create_local_dask_cluster()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis Parameters\n",
    "\n",
    "The following cell sets the parameters, which define the area of interest and the season to conduct the analysis over. The parameters are:\n",
    "\n",
    "* `shp_fpath`: Provide a filepath to a shapefile that defines your AOI, if not using a shapefile then put `None` here.\n",
    "* `lat`, `lon`, `buffer`: If not using a shapefile to define the AOI, then use a latitide, longitude, and buffer to define a query 'box'.\n",
    "* `year`: The year of interest, e.g. `'2018'`\n",
    "* `season`:  The season of interest, e.g `'DJF'`,`'JFM'`, `'FMA'` etc\n",
    "* `name` : A string value used to name the output geotiff, e.g 'NSW'\n",
    "* `dask_chunks` : dictionary of values to chunk the data using dask e.g. `{'x':3000, 'y':3000}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_year = '2020'\n",
    "prediction_quarter = 'MAM'\n",
    "\n",
    "shp_fpath = 'data/mdb_shps/GWYDIR RIVER.shp'\n",
    "lat, lon, buff = -34.958, 150.281, 0.35\n",
    "resolution = (-120,120)\n",
    "dask_chunks = {'x':1000, 'y':1000}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine your area of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map_shapefile(gpd.read_file(shp_fpath), attribute='DNAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display_map(y=(lat-buff, lat + buff), x=(lon-buff, lon + buff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2017, 2020)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate time series of NDVI anomalies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017 MAM\r"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#define the 3-month intervals\n",
    "quarter= {'JFM': [1,2,3],\n",
    "           'FMA': [2,3,4],\n",
    "           'MAM': [3,4,5],\n",
    "           'AMJ': [4,5,6],\n",
    "           'MJJ': [5,6,7],\n",
    "           'JJA': [6,7,8],\n",
    "           'JAS': [7,8,9],\n",
    "           'ASO': [8,9,10],\n",
    "           'SON': [9,10,11],\n",
    "           'OND': [10,11,12],\n",
    "           'NDJ': [11,12,1],\n",
    "           'DJF': [12,1,2],\n",
    "                      }\n",
    "#get years to calculate\n",
    "years_range = int(prediction_year) - 3, int(prediction_year) \n",
    "years = [str(i) for i in range(years_range[0], years_range[1])]\n",
    "\n",
    "#loop through each 3 month period and calculate the anomaly\n",
    "z=[]\n",
    "for year in years:\n",
    "    for q in quarter:\n",
    "        print(year, q, end=\"\\r\")\n",
    "        with HiddenPrints():\n",
    "            anomalies = calculate_anomalies(shp_fpath=shp_fpath,\n",
    "                                query_box=(lat,lon,buff),\n",
    "                                resolution=resolution,\n",
    "                                year=year,\n",
    "                                season=q,\n",
    "                                dask_chunks=dask_chunks).compute()\n",
    "        \n",
    "        z.append(anomalies.rename(year+'_'+q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build back into time-series\n",
    "stand_anomalies=xr.concat(z, dim=pd.date_range(start='2/'+str(years_range[0]), end='1/'+str(years_range[1]), freq='M')).rename({'concat_dim':'time'})\n",
    "\n",
    "stand_anomalies.mean(['x','y']).plot(figsize=(11,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a forecast\n",
    "\n",
    "`AutoReg` doesn't like the all-NaN's slices outide the mask extent, run `stand_anomalies.fillna(-999)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = stand_anomalies.notnull().all('time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mask where its all-NaN's\n",
    "stand_anomalies = stand_anomalies.fillna(-999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_length=1\n",
    "window=20\n",
    "lags=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "def xr_autoregress(da, test_length, window, lags):\n",
    "    #dropna conveneiently with pandas\n",
    "    da =  da[~np.isnan(da)]\n",
    "    # split dataset\n",
    "    train, test = da[1:len(da)-test_length], da[len(da)-test_length:]\n",
    "    # train autoregression\n",
    "    model = AutoReg(train, lags=lags)\n",
    "    model_fit = model.fit()\n",
    "    coef = model_fit.params\n",
    "\n",
    "    # walk forward over time steps in test\n",
    "    history = train[len(train)-window:]\n",
    "    history = [history[i] for i in range(len(history))]\n",
    "\n",
    "    predictions = list()\n",
    "    for t in range(len(test)):\n",
    "        length = len(history)\n",
    "        lag = [history[i] for i in range(length-window,length)]\n",
    "        yhat = coef[0]\n",
    "        for d in range(window):\n",
    "            yhat += coef[d+1] * lag[window-d-1]\n",
    "        obs = test[t]\n",
    "        predictions.append(yhat)\n",
    "        history.append(obs) \n",
    "    \n",
    "    return np.array(predictions).flatten()\n",
    "\n",
    "predict = xr.apply_ufunc(xr_autoregress,\n",
    "                      stand_anomalies, #.chunk(dict(x=750,y=750,time=-1)),\n",
    "                      kwargs={'test_length':test_length,'window':window,'lags':window},\n",
    "                      input_core_dims=[['time']],\n",
    "                      output_core_dims=[['predictions']], \n",
    "                      output_sizes=({'predictions':test_length}),\n",
    "                      exclude_dims=set(('time',)),\n",
    "                      vectorize=True,\n",
    "                      dask=\"parallelized\",\n",
    "                      output_dtypes=[stand_anomalies.dtype]).compute()\n",
    "\n",
    "print(predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = predict.where(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict.plot(size=6, vmin=-2.0, vmax=2, cmap='BrBG')\n",
    "plt.title('Standardised NDVI Anomaly one-month prediction');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stand_anomalies=stand_anomalies.where(mask)\n",
    "stand_anomalies.isel(time=-1).plot(size=6, vmin=-2.0, vmax=2, cmap='BrBG')\n",
    "plt.title('Standardised NDVI Anomaly observation');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = predict - stand_anomalies.isel(time=-1)\n",
    "\n",
    "diff.plot(size=6, vmin=-2.0, vmax=2, cmap='RdBu')\n",
    "plt.title('Difference');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff.mean(['x','y'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
